# Configuration file for Xeon X5550 Gainestown
# See http://en.wikipedia.org/wiki/Gainestown_(microprocessor)#Gainestown
# and http://ark.intel.com/products/37106

#include nehalem

[perf_model/core]
frequency[] = 4.0,4.0,4.0,4.0 #Moved to base.cfg


[scheduler/open/dvfs]
logic = off  # set the DVFS algorithm used. Possible algorithms: off (no DVFS), maxFreq, fixedPower, testStaticPower.
logic = maxFreq  # cfg:maxFreq
#logic = testStaticPower  # cfg:testStaticPower
#min_frequency[] = 3.6,3.6,2.7,2.7
#max_frequency[] = 4.9,4.9,3.8,3.8
min_frequency = 1.0
max_frequency = 4.0
#max_frequency = 3.0  # cfg:3.0GHz
#max_frequency = 4.0  # cfg:4.0GHz
frequency_step_size = 0.1
dvfs_epoch = 1000000
dvfs_epoch = 1000000 # cfg:slowDVFS
#dvfs_epoch = 250000  # cfg:mediumDVFS
#dvfs_epoch = 100000  # cfg:fastDVFS
reserved_cores_are_active = false

[perf_model/core/interval_timer]
dispatch_width = 2
window_size = 64


[perf_model/cache]
levels = 3

[perf_model/l1_icache]
#cache_size[] = 512,512,16,16
cache_size = 16
cache_block_size = 16
associativity = 8
address_hash = mask
replacement_policy = lru
data_access_time = 3
tags_access_time = 1
perf_model_type = parallel
writethrough = 0
shared_cores = 1

[perf_model/l1_dcache]
#cache_size = 32
cache_size = 16
cache_block_size = 16
associativity = 8
address_hash = mask
replacement_policy = lru
data_access_time = 3
tags_access_time = 1
writeback_time = 150 # L2 hit time will be added
perf_model_type = parallel
writethrough = 0
shared_cores = 1

[perf_model/l2_cache]
#cache_size[] = 1024,1024,128,128
cache_size=512
cache_block_size = 16
associativity = 8
address_hash = mod
replacement_policy = lru
data_access_time = 14 # According to http://www.realworldtech.com/page.cfm?ArticleID=RWT040208182719&p=7
tags_access_time = 3
writeback_time = 60 # L3 hit time will be added
perf_model_type = parallel
writethrough = 0
shared_cores = 1


[perf_model/l3_cache]
perfect = false
cache_block_size = 16
cache_size = 8192
associativity = 16
address_hash = mask
replacement_policy = lru
data_access_time = 30 # 35 cycles total according to membench, +L1+L2 tag times
tags_access_time = 10
perf_model_type = parallel
writethrough = 0
shared_cores = 4

[perf_model/dram_directory]
# total_entries = number of entries per directory controller.
total_entries = 1048576
associativity = 16
directory_type = full_map

[perf_model/dram]
# -1 means that we have a number of distributed DRAM controllers (4 in this case)
num_controllers = -1
controllers_interleaving = 4
# DRAM access latency in nanoseconds. Should not include L1-LLC tag access time, directory access time (14 cycles = 5.2 ns),
# or network time [(cache line size + 2*{overhead=40}) / network bandwidth = 18 ns]
# Membench says 175 cycles @ 2.66 GHz = 66 ns total
latency = 45
per_controller_bandwidth = 7.6              # In GB/s, as measured by core_validation-dram
chips_per_dimm = 8
dimms_per_controller = 4

[network]
memory_model_1 = bus
memory_model_2 = bus

[network/bus]
bandwidth = 25.6 # in GB/s. Actually, it's 12.8 GB/s per direction and per connected chip pair
ignore_local_traffic = true # Memory controllers are on-chip, so traffic from core0 to dram0 does not use the QPI links

